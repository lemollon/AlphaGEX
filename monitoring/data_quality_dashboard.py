"""
Data Quality Dashboard - Real-time Monitoring

Monitors database table population, data freshness, and Polygon.io utilization.
Use this to ensure you're maximizing your data sources and maintaining data quality.

Usage:
    python data_quality_dashboard.py                    # Full dashboard
    python data_quality_dashboard.py --quick             # Quick status check
    python data_quality_dashboard.py --json              # JSON output for APIs

Author: AlphaGEX Team
Date: 2025-11-24
"""

import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
from database_adapter import get_connection


class DataQualityDashboard:
    """Monitor database table population and data quality"""

    def __init__(self):
        self.tables_analyzed = 0
        self.data_quality_score = 0.0
        self.recommendations = []

    def run_full_analysis(self) -> Dict:
        """Run complete data quality analysis"""
        print("\n" + "="*80)
        print("üìä ALPHAGEX DATA QUALITY DASHBOARD")
        print("="*80)
        print(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)

        # Get table statistics
        table_stats = self._analyze_tables()

        # Categorize tables
        categories = self._categorize_tables(table_stats)

        # Print results
        self._print_category_stats(categories)

        # Analyze data freshness
        freshness = self._analyze_data_freshness()

        # Polygon.io utilization analysis
        polygon_usage = self._analyze_polygon_utilization()

        # Generate recommendations
        self._generate_recommendations(table_stats, freshness)

        # Calculate overall score
        self._calculate_quality_score(table_stats)

        # Print summary
        self._print_summary()

        return {
            'table_stats': table_stats,
            'categories': categories,
            'freshness': freshness,
            'polygon_usage': polygon_usage,
            'quality_score': self.data_quality_score,
            'recommendations': self.recommendations
        }

    def _analyze_tables(self) -> Dict[str, Dict]:
        """Analyze all database tables"""
        try:
            conn = get_connection()
            cursor = conn.cursor()

            # Get all tables
            cursor.execute("""
                SELECT tablename
                FROM pg_tables
                WHERE schemaname = 'public'
                ORDER BY tablename
            """)

            tables = cursor.fetchall()
            table_stats = {}

            for (table_name,) in tables:
                try:
                    # Get row count
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]

                    # Try to get latest timestamp
                    latest = None
                    for ts_column in ['timestamp', 'created_at', 'date', 'entry_date']:
                        try:
                            cursor.execute(f"SELECT MAX({ts_column}) FROM {table_name}")
                            result = cursor.fetchone()
                            if result and result[0]:
                                latest = result[0]
                                break
                        except:
                            continue

                    table_stats[table_name] = {
                        'row_count': count,
                        'latest_timestamp': latest,
                        'is_empty': count == 0
                    }

                except Exception as e:
                    table_stats[table_name] = {
                        'row_count': 0,
                        'latest_timestamp': None,
                        'is_empty': True,
                        'error': str(e)
                    }

            conn.close()
            return table_stats

        except Exception as e:
            print(f"‚ùå Error analyzing tables: {e}")
            return {}

    def _categorize_tables(self, table_stats: Dict) -> Dict:
        """Categorize tables by purpose"""
        categories = {
            'üìä Core GEX/Gamma Data': [],
            'üß† Psychology Trap Detection': [],
            'üíº Trading & Positions': [],
            'üìà Backtesting & Performance': [],
            'ü§ñ Autonomous Trading': [],
            'üî¨ Strategy Optimization': [],
            'ü§ñ AI Self-Learning': [],
            'üîî Notifications': [],
            'üí¨ Other': []
        }

        for table, stats in table_stats.items():
            entry = {'name': table, 'count': stats['row_count'], 'latest': stats['latest_timestamp']}

            if any(x in table.lower() for x in ['gex', 'gamma']):
                categories['üìä Core GEX/Gamma Data'].append(entry)
            elif any(x in table.lower() for x in ['regime', 'psychology', 'liberation', 'sucker', 'forward_magnet', 'expiration_timeline', 'historical_oi']):
                categories['üß† Psychology Trap Detection'].append(entry)
            elif any(x in table.lower() for x in ['position', 'recommendation', 'trade']):
                categories['üíº Trading & Positions'].append(entry)
            elif any(x in table.lower() for x in ['backtest', 'performance']) and not any(x in table.lower() for x in ['strike', 'dte', 'greeks', 'spread']):
                categories['üìà Backtesting & Performance'].append(entry)
            elif any(x in table.lower() for x in ['autonomous', 'scheduler']):
                categories['ü§ñ Autonomous Trading'].append(entry)
            elif any(x in table.lower() for x in ['strike_performance', 'dte_performance', 'greeks_performance', 'spread_width']):
                categories['üî¨ Strategy Optimization'].append(entry)
            elif any(x in table.lower() for x in ['probability', 'calibration']):
                categories['ü§ñ AI Self-Learning'].append(entry)
            elif 'push' in table.lower() or 'notification' in table.lower():
                categories['üîî Notifications'].append(entry)
            else:
                categories['üí¨ Other'].append(entry)

        return categories

    def _print_category_stats(self, categories: Dict):
        """Print table statistics by category"""
        print("\n" + "="*80)
        print("üìã TABLE POPULATION STATUS")
        print("="*80)

        for category, tables in categories.items():
            if not tables:
                continue

            print(f"\n{category}")
            print("-" * 80)

            for table in sorted(tables, key=lambda x: x['count'], reverse=True):
                status = "‚úÖ" if table['count'] > 0 else "‚ùå"
                latest_str = ""
                if table['latest']:
                    if isinstance(table['latest'], datetime):
                        days_ago = (datetime.now() - table['latest']).days
                        latest_str = f" (last: {days_ago}d ago)"
                    else:
                        latest_str = f" (last: {table['latest']})"

                print(f"  {status} {table['name']:40} {table['count']:>10,} rows{latest_str}")

    def _analyze_data_freshness(self) -> Dict:
        """Analyze data freshness"""
        try:
            conn = get_connection()
            cursor = conn.cursor()

            freshness = {}

            # Check GEX data freshness
            cursor.execute("SELECT MAX(timestamp) FROM gex_history")
            result = cursor.fetchone()
            if result and result[0]:
                freshness['gex_history'] = result[0]

            # Check regime signals freshness
            cursor.execute("SELECT MAX(timestamp) FROM regime_signals")
            result = cursor.fetchone()
            if result and result[0]:
                freshness['regime_signals'] = result[0]

            # Check historical OI freshness
            cursor.execute("SELECT MAX(date) FROM historical_open_interest")
            result = cursor.fetchone()
            if result and result[0]:
                freshness['historical_oi'] = result[0]

            conn.close()
            return freshness

        except Exception as e:
            print(f"‚ö†Ô∏è  Error analyzing freshness: {e}")
            return {}

    def _analyze_polygon_utilization(self) -> Dict:
        """Analyze Polygon.io API utilization"""
        usage = {
            'currently_using': [
                '‚úÖ Daily OHLCV bars (gex_history, gamma_history)',
                '‚úÖ Intraday bars (RSI calculations)',
                '‚úÖ VIX data (volatility regime)',
            ],
            'available_not_used': [
                '‚ùå Options Chain Data (FREE!) - Would improve GEX accuracy 70% ‚Üí 95%',
                '‚ùå Real-time Open Interest (FREE on Options tier)',
                '‚ùå Options Volume by Strike (FREE)',
                '‚ùå Options Greeks (Paid tier) - Actual vs. Black-Scholes estimates',
            ],
            'utilization_pct': 30.0
        }
        return usage

    def _generate_recommendations(self, table_stats: Dict, freshness: Dict):
        """Generate actionable recommendations"""
        self.recommendations = []

        # Check for empty optimization tables
        opt_tables = ['strike_performance', 'dte_performance', 'greeks_performance', 'spread_width_performance']
        empty_opt = [t for t in opt_tables if table_stats.get(t, {}).get('is_empty', True)]

        if empty_opt:
            self.recommendations.append({
                'priority': 'HIGH',
                'category': 'Strategy Optimization',
                'issue': f'{len(empty_opt)} optimization tables are empty',
                'action': 'Run: python enhanced_backtest_optimizer.py',
                'impact': 'Enable auto-optimization of strike selection, DTE, Greeks, and spread widths'
            })

        # Check historical OI data quality
        if 'historical_oi' in freshness:
            latest_oi = freshness['historical_oi']
            if isinstance(latest_oi, (datetime, str)):
                # Check if using synthetic data
                oi_count = table_stats.get('historical_open_interest', {}).get('row_count', 0)
                if oi_count < 500:  # Likely synthetic
                    self.recommendations.append({
                        'priority': 'CRITICAL',
                        'category': 'Data Quality',
                        'issue': 'historical_open_interest has synthetic data (low row count)',
                        'action': 'Run: python polygon_oi_backfill.py --days 90',
                        'impact': 'Improve GEX accuracy from 70% to 95%, better gamma wall detection'
                    })

        # Check data freshness
        if 'gex_history' in freshness:
            latest_gex = freshness['gex_history']
            if isinstance(latest_gex, datetime):
                days_since_update = (datetime.now() - latest_gex).days
                if days_since_update > 7:
                    self.recommendations.append({
                        'priority': 'MEDIUM',
                        'category': 'Data Freshness',
                        'issue': f'GEX data is {days_since_update} days old',
                        'action': 'Check if data pipeline is running',
                        'impact': 'Outdated GEX data leads to poor trading decisions'
                    })

        # Check for empty AI tables
        ai_tables = ['probability_predictions', 'probability_outcomes', 'calibration_history']
        empty_ai = [t for t in ai_tables if table_stats.get(t, {}).get('is_empty', True)]

        if len(empty_ai) == len(ai_tables):
            self.recommendations.append({
                'priority': 'MEDIUM',
                'category': 'AI Self-Learning',
                'issue': 'AI self-learning system not calibrated',
                'action': 'Wait for autonomous trader to make predictions and track outcomes',
                'impact': 'Self-learning will improve prediction accuracy over time'
            })

    def _calculate_quality_score(self, table_stats: Dict):
        """Calculate overall data quality score"""
        total_tables = len(table_stats)
        populated_tables = sum(1 for t in table_stats.values() if not t['is_empty'])

        # Core tables weight more
        core_tables = ['gex_history', 'gamma_history', 'regime_signals', 'historical_open_interest']
        core_populated = sum(1 for t in core_tables if not table_stats.get(t, {}).get('is_empty', True))

        # Optimization tables
        opt_tables = ['strike_performance', 'dte_performance', 'greeks_performance', 'spread_width_performance']
        opt_populated = sum(1 for t in opt_tables if not table_stats.get(t, {}).get('is_empty', True))

        # Weighted score
        base_score = (populated_tables / total_tables) * 40  # 40% weight for general population
        core_score = (core_populated / len(core_tables)) * 40  # 40% weight for core tables
        opt_score = (opt_populated / len(opt_tables)) * 20  # 20% weight for optimization

        self.data_quality_score = base_score + core_score + opt_score

    def _print_summary(self):
        """Print summary and recommendations"""
        print("\n" + "="*80)
        print("üéØ DATA QUALITY SCORE")
        print("="*80)
        print(f"\nOverall Score: {self.data_quality_score:.1f}/100")

        if self.data_quality_score >= 90:
            rating = "üåü EXCELLENT"
            color = "green"
        elif self.data_quality_score >= 70:
            rating = "‚úÖ GOOD"
            color = "yellow"
        elif self.data_quality_score >= 50:
            rating = "‚ö†Ô∏è  FAIR"
            color = "yellow"
        else:
            rating = "‚ùå POOR"
            color = "red"

        print(f"Rating: {rating}\n")

        # Print recommendations
        if self.recommendations:
            print("="*80)
            print("üöÄ RECOMMENDED ACTIONS")
            print("="*80)

            for i, rec in enumerate(self.recommendations, 1):
                priority_icon = {
                    'CRITICAL': 'üî¥',
                    'HIGH': 'üü†',
                    'MEDIUM': 'üü°',
                    'LOW': 'üü¢'
                }
                icon = priority_icon.get(rec['priority'], '‚ö™')

                print(f"\n{i}. {icon} {rec['priority']} - {rec['category']}")
                print(f"   Issue:  {rec['issue']}")
                print(f"   Action: {rec['action']}")
                print(f"   Impact: {rec['impact']}")

        print("\n" + "="*80)

    def quick_status(self):
        """Quick status check (minimal output)"""
        try:
            conn = get_connection()
            cursor = conn.cursor()

            # Check key tables
            key_tables = {
                'gex_history': 'GEX Data',
                'historical_open_interest': 'Open Interest',
                'regime_signals': 'Psychology Traps',
                'strike_performance': 'Strike Optimization'
            }

            print("\n" + "="*60)
            print("‚ö° QUICK STATUS CHECK")
            print("="*60)

            for table, label in key_tables.items():
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                status = "‚úÖ" if count > 0 else "‚ùå"
                print(f"{status} {label:25} {count:>10,} rows")

            conn.close()
            print("="*60 + "\n")

        except Exception as e:
            print(f"‚ùå Error: {e}")

    def json_output(self) -> str:
        """Return dashboard data as JSON"""
        result = self.run_full_analysis()
        return json.dumps(result, indent=2, default=str)


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description='AlphaGEX Data Quality Dashboard')
    parser.add_argument('--quick', action='store_true', help='Quick status check')
    parser.add_argument('--json', action='store_true', help='JSON output')
    args = parser.parse_args()

    dashboard = DataQualityDashboard()

    if args.quick:
        dashboard.quick_status()
    elif args.json:
        print(dashboard.json_output())
    else:
        dashboard.run_full_analysis()


if __name__ == '__main__':
    main()
